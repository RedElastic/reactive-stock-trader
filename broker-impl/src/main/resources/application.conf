play {
  akka.actor-system = "reactivestock-broker"
  http.secret.key = "changeme"
  http.secret.key = ${?APPLICATION_SECRET}
  server.pidfile.path=/dev/null
}

# Enable the serializer provided in Akka 2.5.8+ for akka.Done and other internal
# messages to avoid the use of Java serialization.
akka.actor.serialization-bindings {
  "akka.Done" = akka-misc
  "akka.actor.Address" = akka-misc
  "akka.remote.UniqueAddress" = akka-misc
}

lagom.services {
  portfolio = ${?PORTFOLIO_SERVICE_URL}
}

quote.iex.hostname = "https://sandbox.iexapis.com/stable"
quote.iex.token = "Tpk_e2fc3d830df74ab89916d7b8bdf6b550"

akka.cluster.sharding.state-store-mode = ddata

######################################
# Persistence (Cassandra) Configuration
######################################

broker.cassandra.keyspace = broker

cassandra-journal {
  keyspace = ${broker.cassandra.keyspace}
  keyspace-autocreate = true
  tables-autocreate = true
}

cassandra-snapshot-store {
  keyspace = ${broker.cassandra.keyspace}
  keyspace-autocreate = true
  tables-autocreate = true
}

lagom.persistence.read-side.cassandra {
  keyspace = ${broker.cassandra.keyspace}
  keyspace-autocreate = true
}

cassandra-query-journal {
  eventual-consistency-delay = 200ms
  delayed-event-timeout = 30s
  refresh-interval = 1s
}

lagom.persistence {
  # As a rule of thumb, the number of shards should be a factor ten greater
  # than the planned maximum number of cluster nodes. Less shards than number
  # of nodes will result in that some nodes will not host any shards. Too many
  # shards will result in less efficient management of the shards, e.g.
  # rebalancing overhead, and increased latency because the coordinator is
  # involved in the routing of the first message for each shard. The value
  # must be the same on all nodes in a running cluster. It can be changed
  # after stopping all nodes in the cluster.
  max-number-of-shards = 100

  # Persistent entities saves snapshots after this number of persistent
  # events. Snapshots are used to reduce recovery times.
  # It may be configured to "off" to disable snapshots.
  # Author note: snapshotting turned off
  snapshot-after = off

  # A persistent entity is passivated automatically if it does not receive
  # any messages during this timeout. Passivation is performed to reduce
  # memory consumption. Objects referenced by the entity can be garbage
  # collected after passivation. Next message will activate the entity
  # again, which will recover its state from persistent storage. Set to 0
  # to disable passivation - this should only be done when the number of
  # entities is bounded and their state, sharded across the cluster, will
  # fit in memory.
  # Author note: Set to one day - this may be a bit long for production.
  passivate-after-idle-timeout = 86400s

  # Specifies that entities run on cluster nodes with a specific role.
  # If the role is not specified (or empty) all nodes in the cluster are used.
  # The entities can still be accessed from other nodes.
  run-entities-on-role = ""

  # Default timeout for PersistentEntityRef.ask replies.
  # Author note: Made longer to support potentially slower Minikube environment
  ask-timeout = 60s

  dispatcher {
    type = Dispatcher
    executor = "thread-pool-executor"
    thread-pool-executor {
      fixed-pool-size = 16
    }
    throughput = 1
  }
}

lagom.persistence.read-side {
  # how long should we wait when retrieving the last known offset
  # default is 5s, made longer for POC
  offset-timeout = 60s

  # Exponential backoff for failures in ReadSideProcessor
  failure-exponential-backoff {
    # minimum (initial) duration until processor is started again
    # after failure
    min = 3s

    # the exponential back-off is capped to this duration
    max = 30s

    # additional random delay is based on this factor
    random-factor = 0.2
  }

  # The amount of time that a node should wait for the global prepare callback to execute
  # default is 20s, made longer for POC
  global-prepare-timeout = 60s

  # Specifies that the read side processors should run on cluster nodes with a specific role.
  # If the role is not specified (or empty) all nodes in the cluster are used.
  run-on-role = ""

  # The Akka dispatcher to use for read-side actors and tasks.
  use-dispatcher = "lagom.persistence.dispatcher"
}
